import json
from pathlib import Path
import re

print("[LLM MODULE] llm_classifier.py –∑–∞–≥—Ä—É–∂–µ–Ω")

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
PROMPTS_PATH = Path("config/prompts.json")
with open(PROMPTS_PATH, "r", encoding="utf-8-sig") as f:
    PROMPTS = json.load(f)

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LLM –∏ –±—ç–∫–µ–Ω–¥–∞
with open("config/llm_config.json", "r", encoding="utf-8") as f:
    llm_conf = json.load(f)

llm_backend = llm_conf.get("llm_backend", 0)

if llm_backend == 0:
    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OpenAI")
    from openai import OpenAI
    client = OpenAI(
        base_url=llm_conf["base_url_Open_AI"],
        api_key=llm_conf["api_key_Open_AI"],
    )
elif llm_backend == 1:
    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LangChain+Groq")
    from langchain_groq import ChatGroq
    from langchain_core.messages import HumanMessage
    client = ChatGroq(
        model=llm_conf.get("model_LANGCHAIN", "llama3-8b-8192"),
        api_key=llm_conf["api_key_LANGCHAIN"]
    )
else:
    print("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è LangChain+Mistral")
    from langchain_mistralai import ChatMistralAI
    from langchain_core.messages import HumanMessage

    client = ChatMistralAI(
        model_name=llm_conf.get("model_LANGCHAIN_mistral", "mistral-medium-latest"),
        api_key=llm_conf["api_key_LANGCHAIN_mistral"]
    )

DEPARTMENTS = [
    "–ö–∏–±–µ—Ä–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å",
    "–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –°–µ–∫—Ç–æ—Ä",
    "–ë–∏–∑–Ω–µ—Å",
    "–Æ—Ä–∏—Å—Ç—ã",
    "–ö–æ–º–ø–ª–∞–µ–Ω—Å"
]

import asyncio

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è RPS (1 –∑–∞–ø—Ä–æ—Å –≤ 1.2 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è Mistral Free)
llm_semaphore = asyncio.Semaphore(1)

def clean_department_response(resp: str) -> list[str]:
    resp = resp.strip()
    parts = [item.strip(" '\"*").lower() for item in resp.split(",") if item.strip()]
    departments_lower = [d.lower() for d in DEPARTMENTS]
    result = [DEPARTMENTS[i] for i, d in enumerate(departments_lower) if d in parts]
    return result or ["–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"]

def clean_rating_response(resp: str) -> int:
    match = re.search(r"(-?\d+)", resp)
    if not match:
        return 0
    try:
        val = int(match.group(1))
        return max(1, min(val, 10))
    except ValueError:
        return 0

async def classify_department(news_text: str) -> list[str]:
    prompt = PROMPTS["classification"].format(
        departments=", ".join(DEPARTMENTS),
        text=news_text
    )
    
    max_retries = 5
    current_delay = 2.0
    
    async with llm_semaphore:
        for attempt in range(max_retries + 1):
            try:
                if llm_backend == 0:
                    resp = client.chat.completions.create(
                        model=llm_conf["model_Open_AI"],
                        extra_body={},
                        messages=[{"role": "user", "content": prompt}]
                    )
                    raw = resp.choices[0].message.content
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ainvoke –¥–ª—è LangChain
                    res = await client.ainvoke([HumanMessage(content=prompt)])
                    raw = res.content

                print(f"–°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {raw}")
                # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –ü–û–°–õ–ï —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞, —á—Ç–æ–±—ã –Ω–µ —á–∞—Å—Ç–∏—Ç—å (1.2—Å –¥–ª—è –∑–∞–ø–∞—Å–∞)
                await asyncio.sleep(1.2)
                
                depts = clean_department_response(raw)
                print(f"[DEBUG] –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–æ –≤ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—ã: {depts}")
                return depts
                
            except Exception as e:
                error_str = str(e)
                if "429" in error_str or "rate" in error_str.lower():
                    if attempt < max_retries:
                        print(f"[CLASSIFY] Rate limit (429). Retrying in {current_delay}s... (Attempt {attempt + 1}/{max_retries})")
                        await asyncio.sleep(current_delay)
                        current_delay *= 2
                        continue
                print(f"[ERROR CLASSIFY] {e}")
                return ["–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"]
    
    return ["–ë–µ–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"]

async def rate_engagement(text: str, department: str) -> int:
    prompt = PROMPTS["rating"].format(
        department=department or "–æ–±—â–µ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏",
        text=text
    )
    
    max_retries = 5
    current_delay = 2.0
    
    async with llm_semaphore:
        for attempt in range(max_retries + 1):
            try:
                if llm_backend == 0:
                    resp = client.chat.completions.create(
                        model=llm_conf["model_Open_AI"],
                        extra_body={},
                        messages=[{"role": "user", "content": prompt}],
                    )
                    raw = resp.choices[0].message.content
                else:
                    res = await client.ainvoke([HumanMessage(content=prompt)])
                    raw = res.content

                await asyncio.sleep(1.2)
                rating = clean_rating_response(raw)
                print(f"[DEBUG] –ò–Ω—Ç–µ—Ä–µ—Å–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏: {rating}")
                return rating
                
            except Exception as e:
                error_str = str(e)
                if "429" in error_str or "rate" in error_str.lower():
                    if attempt < max_retries:
                        print(f"[RATING] Rate limit (429). Retrying in {current_delay}s... (Attempt {attempt + 1}/{max_retries})")
                        await asyncio.sleep(current_delay)
                        current_delay *= 2
                        continue
                print(f"[LLM RATING ERROR] {e}")
                return 0
    
    return 0


if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
    test_news = ["–î–º–∏—Ç—Ä–∏–π –î–µ–º–µ—à–∏–Ω –ø–æ—Ä—É—á–∏–ª –∞–∫—Ç–∏–≤–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–±–æ—Ç—É —Å–æ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–º–∏ —Ñ–µ–¥–µ—Ä–∞—Ü–∏—è–º–∏ –≤ –•–∞–±–∞—Ä–æ–≤—Å–∫–æ–º –∫—Ä–∞–µ üìç–ò–∑ 108 –¥–µ–π—Å—Ç–≤—É—é—â–∏—Ö —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Ñ–µ–¥–µ—Ä–∞—Ü–∏–π 100 –∏–º–µ—é—Ç –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏—é –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –≤–∏–¥–∞ —Å–ø–æ—Ä—Ç–∞, 8 –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏. –ö–ª—é—á–µ–≤–æ–π —Ñ–∞–∫—Ç–æ—Ä —Ä–∞–∑–≤–∏—Ç–∏—è —Å–ø–æ—Ä—Ç–∞ ‚Äì –ø—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π. –ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —Å–ø–æ—Ä—Ç–∞ –∫—Ä–∞—è –ø–æ–º–æ–≥–∞–µ—Ç –≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —á–µ–º–ø–∏–æ–Ω–∞—Ç–æ–≤, –ø–µ—Ä–≤–µ–Ω—Å—Ç–≤ –∫—Ä–∞—è –∏ –î–§–û, –≤—Å–µ—Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π. ‚û°Ô∏è–ì–ª–∞–≤–∞ —Ä–µ–≥–∏–æ–Ω–∞ –æ—Ç–º–µ—Ç–∏–ª –Ω–∏–∑–∫—É—é –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Ñ–µ–¥–µ—Ä–∞—Ü–∏–π. 40 —Ñ–µ–¥–µ—Ä–∞—Ü–∏–π —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –ø–æ–Ω–∏–∑–∏–ª–∏ —á–∏—Å–ª–æ –∑–∞–Ω–∏–º–∞—é—â–∏—Ö—Å—è –∏—Ö –≤–∏–¥–æ–º —Å–ø–æ—Ä—Ç–∞, 33 –∏–∑ 100 –∞–∫–∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–Ω—ã—Ö –∏–º–µ—é—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å ¬´–Ω–æ–ª—å¬ª. 48 —Ñ–µ–¥–µ—Ä–∞—Ü–∏–π –∏–º–µ—é—Ç –Ω—É–ª–µ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –º–µ–∂—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã—Ö, –≤—Å–µ—Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö, –º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π, –∫—É–±–∫–æ–≤. –ü–æ —Å–ª–æ–≤–∞–º –î–º–∏—Ç—Ä–∏—è –î–µ–º–µ—à–∏–Ω–∞, –º–µ—Ä—ã –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ —Ç–∞–∫–∏–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è–º –º–æ–≥—É—Ç –±—ã—Ç—å –∂—ë—Å—Ç–∫–∏–º–∏ ‚Äì –≤–ø–ª–æ—Ç—å –¥–æ –ª–∏—à–µ–Ω–∏—è –∞–∫–∫—Ä–µ–¥–∏—Ç–∞—Ü–∏–∏. ‚úîÔ∏è –í–∏—Ü–µ-–≥—É–±–µ—Ä–Ω–∞—Ç–æ—Ä—É –ê—Ä—Ç–µ–º—É –ú–µ–ª—å–Ω–∏–∫–æ–≤—É –∏ –º–∏–Ω—Å–ø–æ—Ä—Ç–∞ –∫—Ä–∞—è –ø–æ—Ä—É—á–µ–Ω–æ –ø—Ä–æ–≤–µ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç—ã —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã—Ö —Ñ–µ–¥–µ—Ä–∞—Ü–∏–π –∏ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –∫–æ–ª–ª–µ–≥–∏—é —Å –∏—Ö —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è–º–∏."]

    print("üîß –¢–µ—Å—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç–∞:")
    depts = classify_department(test_news)
    print(f"–¢–µ—Å—Ç: {depts}  ‚Äî  \"{test_news[:60]}‚Ä¶\"")