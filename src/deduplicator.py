import os
import re
import json
from sentence_transformers import SentenceTransformer, util

# –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
with open('config/config_project.json', 'r', encoding='utf-8-sig') as f:
    cfg = json.load(f)

NEWS_FILE = cfg.get('news_file', 'logs/queue_for_distribution.json')


class Deduplicator:
    def __init__(self, threshold=0.85, news_file=None):
        self.threshold = threshold
        self.news_file = news_file or NEWS_FILE
        self.model = SentenceTransformer('all-MiniLM-L12-v2')
        self.seen_texts = self._load_existing()
        print(f"[DEDUP] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.seen_texts)} —Å—Ç–∞—Ä—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")

    def _preprocess(self, text: str) -> str:
        text = text.lower()
        text = re.sub(r'https?://\S+', '', text)  # —É–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏
        text = re.sub(r'[^\w\s]', '', text)       # —É–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
        text = re.sub(r'\s+', ' ', text)          # —É–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return text.strip()

    def _load_existing(self):
        if not os.path.exists(self.news_file):
            print("[DEDUP] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å—Ç–∞—Ä—Ç —Å –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞")
            return []

        try:
            with open(self.news_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"[DEDUP] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {e}")
            return []

    def _save(self):
        try:
            with open(self.news_file, 'w', encoding='utf-8') as f:
                json.dump(self.seen_texts, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[DEDUP] –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏: {e}")

    def is_duplicate(self, new_text: str) -> bool:
        if not self.seen_texts:
            return False

        new_text_clean = self._preprocess(new_text)
        new_emb = self.model.encode(new_text_clean).tolist()

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ 100 –Ω–æ–≤–æ—Å—Ç—è–º–∏
        recent_entries = self.seen_texts[-1000:]
        old_embs = [entry['embedding'] for entry in recent_entries]

        cosine_scores = util.cos_sim([new_emb], old_embs)[0]
        max_sim = max(cosine_scores).item()

        print(f"[DEDUP] –ú–∞–∫—Å. —Å—Ö–æ–¥—Å—Ç–≤–æ: {max_sim:.3f}")
        return max_sim >= self.threshold

    def add(self, new_text: str):
        new_text_clean = self._preprocess(new_text)
        embedding = self.model.encode(new_text_clean).tolist()

        self.seen_texts.append({
            'text': new_text,
            'embedding': embedding
        })
        self._save()


if __name__ == "__main__":
    print("üîß –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ Deduplicator...")

    test_file = 'logs/test_queue.json'
    cfg['news_file'] = test_file
    NEWS_FILE = test_file

    if os.path.exists(test_file):
        os.remove(test_file)

    dedup = Deduplicator(threshold=0.9)

    news_1 = "–ö–æ–º–ø–∞–Ω–∏—è X –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª–∞ –≤—ã—Ö–æ–¥ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞"
    news_2 = "–ö–æ–º–ø–∞–Ω–∏—è X –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é —Å–≤–æ–µ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞"
    news_3 = "–í—ã—à–µ–ª –æ—Ç—á–µ—Ç –æ–± —É—è–∑–≤–∏–º–æ—Å—Ç—è—Ö –≤ –ü–û –∫–æ–º–ø–∞–Ω–∏–∏ Y"

    print("\n‚û° –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å")
    assert not dedup.is_duplicate(news_1), "news_1 –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–æ–º"
    dedup.add(news_1)

    print("‚û° –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—á—Ç–∏ –¥—É–±–ª–∏–∫–∞—Ç –≤—Ç–æ—Ä–æ–π –Ω–æ–≤–æ—Å—Ç–∏")
    assert dedup.is_duplicate(news_2), "news_2 –¥–æ–ª–∂–Ω–∞ —Å—á–∏—Ç–∞—Ç—å—Å—è –¥—É–±–ª–∏–∫–∞—Ç–æ–º"

    print("‚û° –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç—Ä–µ—Ç—å—é –Ω–æ–≤–æ—Å—Ç—å (—É–Ω–∏–∫–∞–ª—å–Ω—É—é)")
    assert not dedup.is_duplicate(news_3), "news_3 –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω–æ–π"
    dedup.add(news_3)

    print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—à–ª–∏ —É—Å–ø–µ—à–Ω–æ.")

    if os.path.exists(test_file):
        os.remove(test_file)
